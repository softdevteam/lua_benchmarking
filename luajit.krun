import os
import json
import logging
import subprocess
from logging import debug, warn
from krun.vm_defs import (GenericScriptingVMDef)
from krun import EntryPoint
from krun.util import fatal

# Who to mail
MAIL_TO = []

# Maximum number of error emails to send per-run
#MAX_MAILS = 2

DIR = os.getcwd()

VM_ENV = '"{0}/?/init.lua;{0}/?.lua;{0}/?/?.lua;"'.format(os.path.join(DIR, "lualibs"))

def make_env(vmdir):
  basedir = os.path.join(DIR, "builds", vmdir)
  return {
    'LUA_PATH' : VM_ENV + '"{0}/?/init.lua;{0}/?.lua;{0}/?/?.lua;"'.format(basedir)
  }

HEAP_LIMIT = 2097152  # KiB
STACK_LIMIT = 8192  # KiB

VARIANTS = {
    "default-lua": EntryPoint("bench.lua"),
}

N_EXECUTIONS = 15  # Number of fresh process executions.
ITERATIONS_ALL_VMS = 1500
ITERATIONS_NO_JIT = 100
INSTRUMENT = False

class LuaJITVMDef(GenericScriptingVMDef):
    JIT_STATS_MARKER = "@@@ JIT_STATS "
    JIT_STATS_ALL_MARKER = "@@@ JIT_STATS_ALL "

    def __init__(self, vm_path, entry_point=None,
                 subdir=None, env=None, instrument=False):
        GenericScriptingVMDef.__init__(self, vm_path, "iterations_runner.lua",
                                       env=env, instrument=instrument)
        self.iterations_runner = "iterations_runner.lua"

    def run_exec(self, interpreter, iterations, param, heap_lim_k,
                 stack_lim_k, key, key_pexec_idx, force_dir=None,
                 sync_disks=True):
        return self._generic_scripting_run_exec(interpreter, iterations, param,
                                                heap_lim_k, stack_lim_k, key,
                                                key_pexec_idx,
                                                force_dir=force_dir,
                                                sync_disks=sync_disks)
                                                
class OpenRestyVMDef(GenericScriptingVMDef):
    def __init__(self, vm_path, entry_point=None,
                 subdir=None, env=None, instrument=False):
        GenericScriptingVMDef.__init__(self, vm_path, "openresty_runner.lua",
                                       env=env, instrument=instrument)
        self.iterations_runner = "openresty_runner.lua"


    def run_exec(self, interpreter, iterations, param, heap_lim_k,
                 stack_lim_k, key, key_pexec_idx, force_dir=None,
                 sync_disks=True):
        benchmark = key.split(":")[0]
        script_path = self._get_benchmark_path(benchmark, interpreter,
                     
        
                                   force_dir=force_dir)
        args = [self.vm_path, "-c",  os.path.join(DIR, "nginx.conf"), "-g", 
                "\"#@KRUN@benchmark=%s|iters=%d|param=%d|debug=%d|instrument=%d@KRUN@;\"" % 
                (script_path, iterations, param, logging.getLogger().isEnabledFor(logging.DEBUG), self.instrument)]
        
        return self._run_exec(args, heap_lim_k, stack_lim_k, key,
                              key_pexec_idx, sync_disks=sync_disks)
                              
      
    def _run_exec(self, args, heap_lim_k, stack_lim_k, key, key_pexec_idx,
                  bench_env_changes=None, sync_disks=True):
        """ Deals with actually shelling out """

        if bench_env_changes is None:
            bench_env_changes = []

        # Environment *after* user change.
        # Starts minimal, but user change command (i.e. sudo) may introduce more.
        new_user_env = {"PATH": "/bin:/usr/bin"}

        # Apply envs
        self.apply_env_changes(bench_env_changes, new_user_env)

        # Apply platform specific argument transformations.
        args = self.platform.bench_cmdline_adjust(args, new_user_env)

        if self.instrument:
            # we will redirect stderr to this handle
            stderr_file = open(INST_STDERR_FILE, "w")
        else:
            stderr_file = subprocess.PIPE

        if self.dry_run:
            warn("SIMULATED: Benchmark process execution (--dryrun)")
            return ("", "", 0, None)

        #if not self.platform.no_user_change:
        #    self.platform.make_fresh_krun_user()

        wrapper_filename, envlog_filename = \
            self.make_wrapper_script(args, heap_lim_k, stack_lim_k)
        wrapper_args = self._wrapper_args(wrapper_filename)
        debug("Execute wrapper: %s" % (" ".join(wrapper_args)))

        # Do an OS-level sync. Forces pending writes on to the physical disk.
        # We do this in an attempt to prevent disk commits happening during
        # benchmarking.
        if sync_disks:
            self.platform.sync_disks()

        out, err, rc = self._run_exec_popen(wrapper_args, stderr_file)

        if self.instrument:
            stderr_file.close()

        os.unlink(wrapper_filename)
        return out, err, rc, envlog_filename                        


VMS = {
    'Normal': {
        'vm_def': LuaJITVMDef("builds/normal/luajit", env=make_env("normal"), instrument=INSTRUMENT),
        'variants': ['default-lua'],
        'n_iterations': ITERATIONS_ALL_VMS,
    },
    'GC64': {
        'vm_def': LuaJITVMDef("builds/gc64/luajit", env=make_env("gc64"), instrument=INSTRUMENT),
        'variants': ['default-lua'],
        'n_iterations': ITERATIONS_ALL_VMS,
    },
    'NoJIT': {
        'vm_def': LuaJITVMDef("builds/nojit/luajit", env=make_env("nojit")),
        'variants': ['default-lua'],
        'n_iterations': ITERATIONS_NO_JIT,
    },
    'DualNum': {
        'vm_def': LuaJITVMDef("builds/dualnum/luajit", env=make_env("dualnum"), instrument=INSTRUMENT),
        'variants': ['default-lua'],
        'n_iterations': ITERATIONS_ALL_VMS,
    },
    'RaptorJIT': {
        'vm_def': LuaJITVMDef("builds/raptorjit/luajit", env=make_env("raptorjit")),
        'variants': ['default-lua'],
        'n_iterations': ITERATIONS_ALL_VMS,
    },
    'OpenResty': {
        'vm_def': OpenRestyVMDef("builds/openresty/nginx/nginx", env=make_env("openresty")),
        'variants': ['default-lua'],
        'n_iterations': ITERATIONS_ALL_VMS,
    },
    
}

BENCHMARKS = {
    "binarytrees" : 2,
    "nbody" : 10,
    "fasta": 40,
    "richards": 30,
    "spectralnorm": 2,
    "fannkuch_redux": 45,
    "md5": 140,
    "series": 2,
    "luacheck_parser": 1,
    "luacheck": 1,
    "capnproto_encode": 240,
    "capnproto_decode": 400,
    "jsonlua_encode": 10,
    "jsonlua_decode": 200,
    "luafun": 4,
}

# list of "bench:vm:variant"
SKIP = [
    "capnproto_decode:GC64:*", # Sometime crashes with GC64
    "capnproto_decode:RaptorJIT:*", # Sometimes crashes as well
    #"*:Normal:*",
    #"*:GC64:*",
    #"*:NoJIT:*",
    #"*:DualNum:*",
    #"*:RaptorJIT:*",
    #"luacheck:*:*",
    #"luacheck_parser:*:*",
]

# Commands to run before and after each process execution
#
# Environment available for these commands:
#   KRUN_RESULTS_FILE: path to results file.
#   KRUN_LOG_FILE: path to log file.
#   KRUN_ETA_DATUM: time the ETA was computed
#   KRUN_ETA_VALUE: estimated time of completion
#PRE_EXECUTION_CMDS = ["sudo service cron stop"]
#POST_EXECUTION_CMDS = ["sudo service cron start"]

# CPU pinning (off by default)
ENABLE_PINNING = False

# Lower and upper bound for acceptable APERF/MPERF ratios
#AMPERF_RATIO_BOUNDS = 0.995, 1.005

# Rough rate of change in APERF per second above which a core is considered busy.
# For many machines this is simply the base clock frequency, but generally
# speaking, is undefined, so you need to check on a per-machine basis.
#AMPERF_BUSY_THRESHOLD = 3.4 * 1000 * 1000 * 1000 / 1000  # 3.4 GHz / 1000
